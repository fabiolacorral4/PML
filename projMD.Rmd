---
title: "Pracical Mahine Learning: Course Project"
author: "Fabiola"
date: "9/22/2020"
output: html_document
---
# Project description
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The five ways are exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Only Class A corresponds to correct performance. The goal of this project is to predict the manner in which they did the exercise (Clss A-E). More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see  Weight Lifting Exercise Dataset section).


# Loading of libraries
```{r, results="hide"}
library(caret)
library(rattle)
library(rpart)
library(randomForest)
library(corrplot)
library(e1071)
library(gbm)
```

# Data Processing
After having loaded the necessary libraries, we read in the csv files with the 
training and testing data. The training dataset was originally composed of 160 
variables and 19622 observations, and the testing dataset was composed of 160 
variables and 20 observations. We noticed that the first 7 variables were timestamps, 
and thus not relevant to our analysis, as well as that many variables had mostly
observations with NA values. We excluded these variables from our train and test 
datasets, leaving only 53 variables in both the train and test datasets.
We then proceded to further partition the training dataset into a training and a 
cross-validation set in order to get out of sample errors and prevent 
over-fitting (using a 60-40 ratio).  
```{r}
# read data
data = read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
str(data); dim(data)
testData = read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))
head(testData); dim(testData)
# clean out variables with NA values 
# and columns not relevant to motion
data = data[,colSums(is.na(data))==0]
data = data[,-c(1:7)]
dim(data); names(data)
testData = testData[,colSums(is.na(testData))==0]
testData = testData[,-c(1:7)]
dim(testData); names(testData)
# partition training and cv dataset
set.seed(4444)
inTrain = createDataPartition(data$classe, p=.6, list=FALSE)
dataTrain = data[inTrain,]
dataTest = data[-inTrain,]
dim(dataTrain)
```

# Testing different models
We then continued by testing three different machine learning models 
(classification tree, random forest, gradient boosting method) to test accuracy 
in predicting the "classe" variable. 


## 1. Classification tree
We began our model analysis by using the classification tree model (we set k=5
in our k-fold cross-validation). The results of this first model (accuracy = 0.55) 
seem to indicate that a classification tree is not the optimal model for predicting
"classe" with this dataset.
```{r}
control = trainControl(method="cv", number=5)
mdlCT = train(classe~., data=dataTrain, method="rpart", trControl=control)
print(mdlCT, digits=4)
fancyRpartPlot(mdlCT$finalModel, main = "Classification Tree model")
predCT = predict(mdlCT, dataTest)
confusionMatrix(as.factor(dataTest$classe), predCT)$table
confusionMatrix(as.factor(dataTest$classe), predCT)$overall[1]
```


# 2. Random forest
We continued by trying the Random forest model, which (albeit being mroe time-consuming)
generally performs better due to it being an ensemble learning method that constructs 
a multitude of regression trees and outputs the mean prediction of the individual trees. 
The accuracy of "classe" prediction using this model turned out to be .99, making 
this model an excellent option, with the optimal number of predictors being 23. Since
this model excludes certain predictors, we can assume that there is covariance among a number
of them, making these predictors unecessary for predicting "classe."
```{r}
mdlRF = train(classe~., data=dataTrain, method="rf", trControl=control, verbose=FALSE)
print(mdlRF, digits=4)
names(mdlRF$finalModel)
varImp(mdlRF)
par(mfrow=c(1,2))
plot(mdlRF, main="Effect of number of predictors \n on Random Forest model accuracy")
plot(mdlRF$finalModel, main="Class error \n by number of trees on RF model")
predRF = predict(mdlRF, dataTest)
confusionMatrix(as.factor(dataTest$classe), predRF)$table
confusionMatrix(as.factor(dataTest$classe), predRF)$overall[1]
```

# 3. Gradient boosting method
Finally, we modeled "classe" using the Gradient boosting method, which is another method
for creating an ensemble of machine learning models, but one which uses a variety of models,
not just decision trees. Although the resulting accuracy (.95) is not as high as Random forest, 
it is still very high, suggesting the GBM could also be a good method for predicting the "classe"
variable.
```{r}
mdlGBM = train(classe~., data=dataTrain, method="gbm", trControl=control, verbose=FALSE)
print(mdlGBM, digits=4)
par(mfrow=c(1,1))
plot(mdlGBM)
predGBM = predict(mdlGBM, dataTest)
confusionMatrix(as.factor(dataTest$classe), predGBM)$table
confusionMatrix(as.factor(dataTest$classe), predGBM)$overall[1]
```

# Final testing
Since the Random forest model resulted in the highest accuracy, we will use it to
predict the "classe" variable in the test dataset.
```{r}
predict(mdlRF, testData)
```


